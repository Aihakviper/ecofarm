{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05887668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b2945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_h = 180\n",
    "img_w = 180\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "097f636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55 files belonging to 1 classes.\n",
      "Using 44 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_df = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"cow\",\n",
    "    validation_split = 0.2,\n",
    "    subset= 'training',\n",
    "    seed= 123,\n",
    "    image_size = (img_h, img_w),\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5fb2231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55 files belonging to 1 classes.\n",
      "Using 11 files for validation.\n"
     ]
    }
   ],
   "source": [
    "valid_df = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"cow\",\n",
    "    validation_split = 0.2,\n",
    "    subset = \"validation\",\n",
    "    seed=123,\n",
    "    image_size = (img_h,img_w),\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ed7571",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aug =  tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.4),\n",
    "    layers.RandomZoom(0.3),\n",
    "    layers.RandomContrast(0.1),\n",
    "    layers.RandomBrightness(0.1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08413310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_11512\\945699244.py:3: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(weights='imagenet',input_shape=(img_h,img_w,3),include_top=False)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "base_model = MobileNetV2(weights='imagenet',input_shape=(img_h,img_w,3),include_top=False)\n",
    "\n",
    "base_model.trainable= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b7aa24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cl = len(train_df.class_names)\n",
    "\n",
    "model = Sequential(\n",
    "    [ \n",
    "        data_aug,\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(64,activation='relu'),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Dense(num_cl,activation='sigmoid')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8147e225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_3      │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_3      │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),\n",
    "            #   loss=tf.keras.losses.binarycrossentropy(),\n",
    "            loss='binary_crossentropy',\n",
    "              metrics= ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac636c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7256dc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 8s/step - accuracy: 0.4318 - loss: 0.9256 - val_accuracy: 1.0000 - val_loss: 0.0014 - learning_rate: 0.0020\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 843ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 1.6674e-05 - learning_rate: 0.0020\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 930ms/step - accuracy: 1.0000 - loss: 8.9417e-05 - val_accuracy: 1.0000 - val_loss: 5.5352e-07 - learning_rate: 0.0020\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 4.5601e-06 - val_accuracy: 1.0000 - val_loss: 3.5540e-08 - learning_rate: 0.0020\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 864ms/step - accuracy: 1.0000 - loss: 1.4394e-07 - val_accuracy: 1.0000 - val_loss: 2.2989e-08 - learning_rate: 4.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 999ms/step - accuracy: 1.0000 - loss: 1.3040e-07 - val_accuracy: 1.0000 - val_loss: 1.6193e-08 - learning_rate: 4.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 886ms/step - accuracy: 1.0000 - loss: 1.2830e-06 - val_accuracy: 1.0000 - val_loss: 1.5293e-08 - learning_rate: 8.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 871ms/step - accuracy: 1.0000 - loss: 4.4390e-07 - val_accuracy: 1.0000 - val_loss: 1.4591e-08 - learning_rate: 8.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 816ms/step - accuracy: 1.0000 - loss: 2.0752e-07 - val_accuracy: 1.0000 - val_loss: 1.4479e-08 - learning_rate: 1.6000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 960ms/step - accuracy: 1.0000 - loss: 1.0622e-06 - val_accuracy: 1.0000 - val_loss: 1.4386e-08 - learning_rate: 1.6000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 991ms/step - accuracy: 1.0000 - loss: 1.6499e-06 - val_accuracy: 1.0000 - val_loss: 1.4370e-08 - learning_rate: 3.2000e-06\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 881ms/step - accuracy: 1.0000 - loss: 4.4252e-07 - val_accuracy: 1.0000 - val_loss: 1.4357e-08 - learning_rate: 3.2000e-06\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 882ms/step - accuracy: 1.0000 - loss: 6.1828e-07 - val_accuracy: 1.0000 - val_loss: 1.4355e-08 - learning_rate: 6.4000e-07\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 4.5381e-08 - val_accuracy: 1.0000 - val_loss: 1.4353e-08 - learning_rate: 6.4000e-07\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 860ms/step - accuracy: 1.0000 - loss: 7.9179e-08 - val_accuracy: 1.0000 - val_loss: 1.4353e-08 - learning_rate: 1.2800e-07\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 831ms/step - accuracy: 1.0000 - loss: 2.1683e-07 - val_accuracy: 1.0000 - val_loss: 1.4353e-08 - learning_rate: 1.2800e-07\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 799ms/step - accuracy: 1.0000 - loss: 1.6122e-07 - val_accuracy: 1.0000 - val_loss: 1.4353e-08 - learning_rate: 2.5600e-08\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 917ms/step - accuracy: 1.0000 - loss: 7.7156e-07 - val_accuracy: 1.0000 - val_loss: 1.4353e-08 - learning_rate: 2.5600e-08\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 836ms/step - accuracy: 1.0000 - loss: 3.0534e-06 - val_accuracy: 1.0000 - val_loss: 1.4353e-08 - learning_rate: 5.1200e-09\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 768ms/step - accuracy: 1.0000 - loss: 1.5281e-07 - val_accuracy: 1.0000 - val_loss: 1.4353e-08 - learning_rate: 5.1200e-09\n"
     ]
    }
   ],
   "source": [
    "his = model.fit(train_df, validation_data=valid_df, epochs=20, callbacks=[early, lr_reduce])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "# Freeze most layers — only keep last 50 trainable\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)\n",
    "history2 = model.fit(train_df, validation_data=valid_df, epochs=20, callbacks=[early, lr_reduce])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b0e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "# Freeze most layers — only keep last 50 trainable\n",
    "for layer in base_model.layers[:110]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-6),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history3 = model.fit(train_df, validation_data=valid_df, epochs=20, callbacks=[early, lr_reduce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0703096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "class_names = train_df.class_names\n",
    "\n",
    "def prep_img(imf_path,img_h,img_w):\n",
    "  img= image.load_img(img_path,target_size=(img_h,img_w))\n",
    "  img_arr = image.img_to_array(img)\n",
    "  img_arr = np.expand_dims(img_arr,axis=0)\n",
    "  return img_arr\n",
    "# C:\\Users\\Dell\\Documents\\python projects\\deep tech\\boat data\\train\\20130412_122808_99204.jpg\n",
    "# C:\\Users\\Dell\\Documents\\python projects\\deep tech\\boat data\\train\\20130412_122352_97982.jpg\n",
    "img_path = \"boat data/train/VaporettoACTV2.jpeg\"\n",
    "img_arr = prep_img(img_path,img_h,img_w)\n",
    "prediction = model.predict(img_arr)\n",
    "# prediction\n",
    "pred_class_idx = np.argmax(prediction, axis=1)[0]\n",
    "pred_class_name = class_names[pred_class_idx]\n",
    "pred_class_name\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
